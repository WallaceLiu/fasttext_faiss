{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容\n",
    "- 利用 fasttext 对语料 text_corpus.txt 训练模型，保存 ft_skipgram_ws5_dim64.bin\n",
    "- 利用 tensorflow 对 questions.csv 进行word to ids 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import learn\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    try:\n",
    "        tk_x = x.lower()\n",
    "\n",
    "        # list of characters which needs to be replaced with space\n",
    "        space_replace_chars = ['?', ':', ',', '\"', '[', ']', '~', '*', ';', '!', '?', '(', ')', '{', '}', '@', '$',\n",
    "                               '#', '.', '-', '/']\n",
    "        tk_x = tk_x.translate({ord(x): ' ' for x in space_replace_chars})\n",
    "\n",
    "        non_space_replace_chars = [\"'\"]\n",
    "        tk_x = tk_x.translate({ord(x): '' for x in non_space_replace_chars})\n",
    "\n",
    "        # remove non-ASCII chars\n",
    "        tk_x = ''.join([c if ord(c) < 128 else '' for c in tk_x])\n",
    "\n",
    "        # replace all consecutive spaces with one space\n",
    "        tk_x = re.sub('\\s+', ' ', tk_x).strip()\n",
    "\n",
    "        # find all consecutive numbers present in the word, first converted numbers to * to prevent conflicts while replacing with numbers\n",
    "        regex = re.compile(r'([\\d])')\n",
    "        tk_x = regex.sub('*', tk_x)\n",
    "        nos = re.findall(r'([\\*]+)', tk_x)\n",
    "        # replace the numbers with the corresponding count like 123 by 3\n",
    "        for no in nos:\n",
    "            tk_x = tk_x.replace(no, \"<NUMBER>\", 1)\n",
    "\n",
    "        return tk_x.strip().lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def build_corpus(filepath):\n",
    "    similar_items = pd.read_csv(filepath)\n",
    "    selected_cols = ['question1', 'question2', 'is_duplicate']\n",
    "    similar_items = similar_items[selected_cols]\n",
    "    similar_items['question1'] = similar_items['question1'].apply(preprocess)\n",
    "    similar_items['question2'] = similar_items['question2'].apply(preprocess)\n",
    "    similar_items = shuffle(similar_items)\n",
    "    similar_items = similar_items.drop_duplicates()\n",
    "    question_list = list(similar_items['question1'])\n",
    "    question_list.extend(list(similar_items['question2']))\n",
    "    pd.DataFrame(question_list).to_csv('./data_repository/text_corpus.txt', index=False)\n",
    "    print('Text corpus generated and persisted successfully.')\n",
    "    return similar_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text corpus generated and persisted successfully.\n",
      "FastText training finished successfully.\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = fasttext.train_unsupervised(\"./data_repository/text_corpus.txt\", model='skipgram',\n",
    "                                                    lr=0.1, dim=64,\n",
    "                                                    ws=5, epoch=50)\n",
    "embeddings_model.save_model(\"./model_siamese_network/ft_skipgram_ws5_dim64.bin\")\n",
    "print('FastText training finished successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = build_corpus('./data_repository/questions.csv')\n",
    "current_index = 0\n",
    "\n",
    "input_X = list(similar_pairs['question1'])\n",
    "input_Y = list(similar_pairs['question2'])\n",
    "\n",
    "wc_list_x = list(len(x.split(' ')) for x in input_X)\n",
    "wc_list_y = list(len(x.split(' ')) for x in input_Y)\n",
    "wc_list = []\n",
    "wc_list.extend(wc_list_x)\n",
    "wc_list.extend(wc_list_y)\n",
    "\n",
    "number_of_elements = len(input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metadata(filename, labels):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Index\\tLabel\\n\")\n",
    "        for index, label in enumerate(labels):\n",
    "            f.write(\"{}\\t{}\\n\".format(index, label))\n",
    "\n",
    "    print('Metadata file saved in {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b4430aa95128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1.首先将列表里面的词生成一个词典；\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2.按列表中的顺序给每一个词进行排序，每一个词都对应一个序号(从1开始，<UNK>的序号为0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 3.按照原始列表顺序，将原来的词全部替换为它所对应的序号\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "# tensorflow 1.8\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 建立word到idx的映射关系\n",
    "# 1.首先将列表里面的词生成一个词典；\n",
    "# 2.按列表中的顺序给每一个词进行排序，每一个词都对应一个序号(从1开始，<UNK>的序号为0)\n",
    "# 3.按照原始列表顺序，将原来的词全部替换为它所对应的序号\n",
    "# 4.同时如果大于最大长度的词将进行剪切，小于最大长度的词将进行填充\n",
    "# 5.然后将其转换为列表，进而转换为一个array\n",
    "\n",
    "# 我们使用这些索引值做embedding，然后才能将数据转换成神经网络需要的格式\n",
    "\n",
    "# or use a constant like 16, select this parameter based on your understanding of what could be a good choice\n",
    "max_document_length = 16 \n",
    "# Create the vocabularyprocessor object, setting the max lengh of the documents.\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "full_corpus = [] # 词的list\n",
    "full_corpus.extend(input_X)\n",
    "full_corpus.extend(input_Y)\n",
    "# Transform the documents using the vocabulary.\n",
    "full_data = np.asarray(list(vocab_processor.fit_transform(full_corpus)))\n",
    "\n",
    "embeddings_lookup = []\n",
    "# Extract word:id mapping from the object.\n",
    "for word in list(vocab_processor.vocabulary_._mapping):\n",
    "    try:\n",
    "        embeddings_lookup.append(embeddings_model[str(word)])\n",
    "    except:\n",
    "        pass\n",
    "embeddings_lookup = np.asarray(embeddings_lookup)\n",
    "vocab_processor.save('./model_siamese_network/vocab')\n",
    "write_metadata(os.path.join('model_siamese_network', 'metadata.tsv'), list(vocab_processor.vocabulary_._mapping))\n",
    "print('Vocab processor executed and saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data[0:number_of_elements]\n",
    "Y = full_data[number_of_elements:2 * number_of_elements]\n",
    "label = list(similar_pairs['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_siamese_batch\n",
    "topN=10\n",
    "last_index = current_index\n",
    "current_index += topN\n",
    "X[last_index: current_index, :]\n",
    "Y[last_index: current_index, :]\n",
    "np.expand_dims(label[last_index: current_index], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}